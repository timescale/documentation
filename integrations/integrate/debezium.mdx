---
title: Integrate Debezium with Tiger Cloud
sidebarTitle: Debezium
description: Integrate Debezium with Tiger Cloud to enable change data capture in your Tiger Cloud service and streaming to Redis Streams
keywords: [Debezium, change data capture, CDC, data replication, streaming, real-time sync, database events, Kafka, log-based replication]
---

import { CLOUD_LONG, TIMESCALE_DB, SELF_LONG_CAP, SERVICE_LONG, HYPERTABLE_CAP, HYPERCORE_CAP, COLUMNSTORE, ROWSTORE } from '/snippets/vars.mdx';
import IntegrationPrereqsSelfOnly from '/snippets/prerequisites/_integration-prereqs-self-only.mdx';
import IntegrationDebeziumDocker from '/snippets/integrations/_integration-debezium-docker.mdx';
import IntegrationDebeziumSelfHostedConfig from '/snippets/integrations/_integration-debezium-self-hosted-config-database.mdx';

[Debezium][debezium] is an open-source distributed platform for change data capture (CDC).
It enables you to capture changes in a {SELF_LONG} instance and stream them to other systems in real time.

Debezium can capture events about:

- [{HYPERTABLE_CAP}s][hypertables]: captured events are rerouted from their chunk-specific topics to a single logical topic
   named according to the following pattern: `<topic.prefix>.<hypertable-schema-name>.<hypertable-name>`
- [{CAGG_CAP}s][caggs]: captured events are rerouted from their chunk-specific topics to a single logical topic
  named according to the following pattern: `<topic.prefix>.<aggregate-schema-name>.<aggregate-name>`
- [{HYPERCORE_CAP}][hypercore]: if you enable {HYPERCORE_CAP}, the Debezium {TIMESCALE_DB} connector does not apply any special
  processing to data in the {COLUMNSTORE}. Compressed chunks are forwarded unchanged to the next downstream job in the
  pipeline for further processing as needed. Typically, messages with compressed chunks are dropped, and are not
  processed by subsequent jobs in the pipeline.

   This limitation only affects changes to chunks in the {COLUMNSTORE}. Changes to data in the {ROWSTORE} work correctly.


This page explains how to capture changes in your database and stream them using Debezium on Apache Kafka.

## Prerequisites

<IntegrationPrereqsSelfOnly />

- [Install Docker][install-docker] on your development machine.

## Configure your database to work with Debezium

<Tabs>

<Tab title={`${SELF_LONG_CAP}`}>

To set up {SELF_LONG} to communicate with Debezium:

<IntegrationDebeziumSelfHostedConfig />

## Configure Debezium to work with your database

Set up Kafka Connect server, plugins, drivers, and connectors:

<IntegrationDebeziumDocker />

And that is it,  you have configured Debezium to interact with {TIMESCALE_DB}.

</Tab>

<Tab title={`${CLOUD_LONG}`}>

Debezium requires logical replication to be enabled. Currently, this is not enabled by default on {SERVICE_LONG}s.
We are working on enabling this feature as you read. As soon as it is live, these docs will be updated.

</Tab>

</Tabs>

[caggs]: /use-timescale/continuous-aggregates
[debezium]: https://debezium.io/
[hypercore]: /use-timescale/hypercore
[hypertables]: /use-timescale/hypertables
[install-docker]: https://docs.docker.com/engine/install/