---
title: Integrate Apache Airflow with Tiger Cloud
sidebarTitle: Apache Airflow
description: Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. Integrate Apache Airflow with Tiger Cloud and create a data pipeline
products: [cloud, self_hosted]
keywords: [connect, integrate, apache, airflow]
---

import { CLOUD_LONG, CONSOLE, PG } from '/snippets/vars.mdx';
import IntegrationPrereqs from "/snippets/prerequisites/_integration-prereqs.mdx";

Apache AirflowÂ® is a platform created by the community to programmatically author, schedule, and monitor workflows.

A [DAG (Directed Acyclic Graph)][Airflow-DAG] is the core concept of Airflow, collecting [Tasks][Airflow-Task] together,
organized with dependencies and relationships to say how they should run. You declare a DAG in a Python file
in the `$AIRFLOW_HOME/dags` folder of your Airflow instance.

This page shows you how to use a Python connector in a DAG to integrate Apache Airflow with a {SERVICE_LONG}.

## Prerequisites

<IntegrationPrereqs />

* Install [Python3 and pip3][install-python-pip] 
* Install [Apache Airflow][install-apache-airflow]

   Ensure that your Airflow instance has network access to {CLOUD_LONG}.

This example DAG uses the `company` table you create in [Optimize time-series data in hypertables][create-a-table-in-timescale]

## Install python connectivity libraries

To install the Python libraries required to connect to {CLOUD_LONG}:

<Procedure>

1. **Enable {PG} connections between Airflow and {CLOUD_LONG}**

    ```bash
    pip install psycopg2-binary
    ```

1. **Enable {PG} connection types in the Airflow UI**

    ```bash
    pip install apache-airflow-providers-postgres
    ```
   
</Procedure>

## Create a connection between Airflow and your {SERVICE_LONG}

In your Airflow instance, securely connect to your {SERVICE_LONG}:

<Procedure>

1.  **Run Airflow**

    On your development machine, run the following command:

    ```bash
    airflow standalone
    ```

    The username and password for Airflow UI are displayed in the `standalone | Login with username`
    line in the output.

1. **Add a connection from Airflow to your {SERVICE_LONG}**

   1. In your browser, navigate to `localhost:8080`, then select `Admin` > `Connections`.
   1. Click `+` (Add a new record), then use your [connection info][connection-info] to fill in 
      the form. The `Connection Type` is `Postgres`.

</Procedure> 

## Exchange data between Airflow and your {SERVICE_LONG}
 
To exchange data between Airflow and your {SERVICE_LONG}:

<Procedure>

1. **Create and execute a DAG** 

   To insert data in your {SERVICE_LONG} from Airflow:
   1. In `$AIRFLOW_HOME/dags/timescale_dag.py`, add the following code:

       ```python
       from airflow import DAG
       from airflow.operators.python_operator import PythonOperator
       from airflow.hooks.postgres_hook import PostgresHook
       from datetime import datetime
   
       def insert_data_to_timescale():
           hook = PostgresHook(postgres_conn_id='the ID of the connenction you created')
           conn = hook.get_conn()
           cursor = conn.cursor()
           """
             This could be any query. This example inserts data into the table
             you create in:
      
             https://docs.tigerdata.com/getting-started/latest/try-key-features-timescale-products/#optimize-time-series-data-in-hypertables
            """            
           cursor.execute("INSERT INTO crypto_assets (symbol, name) VALUES (%s, %s)",
            ('NEW/Asset','New Asset Name'))
           conn.commit()
           cursor.close()
           conn.close()
   
       default_args = {
           'owner': 'airflow',
           'start_date': datetime(2023, 1, 1),
           'retries': 1,
       }
   
       dag = DAG('timescale_dag', default_args=default_args, schedule_interval='@daily')
   
       insert_task = PythonOperator(
           task_id='insert_data',
           python_callable=insert_data_to_timescale,
           dag=dag,
       )
       ```
      This DAG uses the `company` table created in [Create regular {PG} tables for relational data][create-a-table-in-timescale].

   1.  In your browser, refresh the Airflow UI.
   1.  In `Search DAGS`, type `timescale_dag` and press ENTER.  
   1.  Press the play icon and trigger the DAG:
       ![daily eth volume of assets](https://assets.timescale.com/docs/images/integrations-apache-airflow.png)
1. **Verify that the data appears in {CLOUD_LONG}** 

   1. In [{CONSOLE}][console], navigate to your service and click `SQL editor`. 
   1. Run a query to view your data. For example: `SELECT symbol, name FROM company;`. 
   
      You see the new rows inserted in the table.

</Procedure>

You have successfully integrated Apache Airflow with {CLOUD_LONG} and created a data pipeline.


[create-a-table-in-timescale]: /getting-started/:currentVersion:/try-key-features-timescale-products/#optimize-time-series-data-in-hypertables
[install-apache-airflow]: https://airflow.apache.org/docs/apache-airflow/stable/start.html
[install-python-pip]: https://docs.python.org/3/using/index.html
[console]: https://console.cloud.timescale.com/
[create-service]: /cloud/get-started/create-services
[enable-timescaledb]: /self-hosted/:currentVersion:/install/
[connection-info]: /integrations/:currentVersion:/find-connection-details/
[Airflow-DAG]: https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#dags
[Airflow-Task]:https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html
[Airflow_UI]: localhost:8080
