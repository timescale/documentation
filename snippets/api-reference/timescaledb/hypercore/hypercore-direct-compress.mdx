When you set `timescaledb.enable_direct_compress_copy` your data gets compressed in memory during ingestion with `COPY` statements.
By writing the compressed batches immediately in the {COLUMNSTORE}, the IO footprint is significantly lower.
Also, the [columnstore policy][add_columnstore_policy] you set is less important, `INSERT` already produces compressed {CHUNK}s.

<Note>
Please note that this feature is a **tech preview** and not production-ready.
Using this feature could lead to regressed query performance and/or storage ratio, if the ingested batches are not
correctly ordered or are of too high cardinality.
</Note>

To enable in-memory data compression during ingestion:

```sql
SET timescaledb.enable_direct_compress_copy=on;
```

**Important facts**
- High cardinality use cases do not produce good batches and lead to degreaded query performance.
- The {COLUMNSTORE} is optimized to store 1000 records per batch, which is the optimal format for ingestion per segment by.
- WAL records are written for the compressed batches rather than the individual tuples.
- Currently only `COPY` is support, `INSERT` will eventually follow.
- Best results are achieved for batch ingestion with 1000 records or more, upper boundary is 10.000 records.
- Continous Aggregates are **not** supported at the moment.

[add_columnstore_policy]: /api-reference/timescaledb/hypercore/add_columnstore_policy
